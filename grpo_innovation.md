# GRPO and Log-Ratio-Based Regularization: Key Concepts

## ğŸ§  Why the Log-Ratio Acts as Regularization

- The log-ratio grows large when the new model makes different choices than the reference model.
- That means low overlap and high variance.
- PPO addresses this by clamping the ratio and adding a KL penalty to avoid wild updates.
- GRPO instead uses the **log-ratio of probabilities** directly in the loss.

### Why is this helpful?
- The **log-ratio is small** when the new model is close to the reference â†’ gradients are small unless it strongly prefers something.
- It **naturally penalizes large changes** â€” the log of a small probability ratio is very negative.
- This creates **cautious updates** unless the reward is high.
- âœ… Built-in regularization: **no need for a separate KL term**.

---

## ğŸ“Œ Importance Sampling: The Problem It Solves

- Goal: Estimate an expected value under one distribution \( P(x) \), but you only have samples from another \( Q(x) \).
- Solution: **Reweight** each sample by \( \frac{P(x)}{Q(x)} \)

### In GRPO:
- \( P(x) \) = generation from the **current model** (Ï€Î¸)
- \( Q(x) \) = generation from the **reference model** (Ï€_ref)
- This reweighting is called **importance sampling**.

### Why it matters:
- You collect data using Ï€_ref but want to train Ï€Î¸.
- So you ask: â€œHow likely is this same sample under the new policy vs the old one?â€
- The ratio \( \frac{P(x)}{Q(x)} \) tells you how much to boost or shrink \( f(x) \) (reward).
- Since you're optimizing P, it's in the numerator.

---

## âœ… GRPOâ€™s Rewarded Log-Ratio Trick

If:
- Reward \( r(x) \) is high
- \( \pi_\theta(x) \gg \pi_{\text{ref}}(x) \)

Then:
- The log-ratio is positive
- Reward is positive
- Loss = \( -r(x) \cdot \log(\pi_\theta / \pi_{\text{ref}}) \) is **negative**
- We minimize the loss â†’ we **reinforce** this behavior.

âœ… So GRPO **encourages divergence from the reference** model **only if the reward justifies it**.

ğŸ§  This allows new behaviors to emerge without needing to balance a KL penalty manually.

---

## âš ï¸ Important Notes

- We **do not** take argmax from both models â€” that would compare different tokens.
- We **only evaluate** the sequence generated by the current model (Ï€Î¸).
- For each token \( x_t \):
  - Take \( \log \pi_\theta(x_t \mid x_{<t}) \)
  - Take \( \log \pi_{\text{ref}}(x_t \mid x_{<t}) \)
  - Compute the log-ratio.
- Both models must share the **same vocabulary index** for this to work.