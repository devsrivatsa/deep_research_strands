# GRPO and Log-Ratio-Based Regularization: Key Concepts

## 🧠 Why the Log-Ratio Acts as Regularization

- The log-ratio grows large when the new model makes different choices than the reference model.
- That means low overlap and high variance.
- PPO addresses this by clamping the ratio and adding a KL penalty to avoid wild updates.
- GRPO instead uses the **log-ratio of probabilities** directly in the loss.

### Why is this helpful?
- The **log-ratio is small** when the new model is close to the reference → gradients are small unless it strongly prefers something.
- It **naturally penalizes large changes** — the log of a small probability ratio is very negative.
- This creates **cautious updates** unless the reward is high.
- ✅ Built-in regularization: **no need for a separate KL term**.

---

## 📌 Importance Sampling: The Problem It Solves

- Goal: Estimate an expected value under one distribution \( P(x) \), but you only have samples from another \( Q(x) \).
- Solution: **Reweight** each sample by \( \frac{P(x)}{Q(x)} \)

### In GRPO:
- \( P(x) \) = generation from the **current model** (πθ)
- \( Q(x) \) = generation from the **reference model** (π_ref)
- This reweighting is called **importance sampling**.

### Why it matters:
- You collect data using π_ref but want to train πθ.
- So you ask: “How likely is this same sample under the new policy vs the old one?”
- The ratio \( \frac{P(x)}{Q(x)} \) tells you how much to boost or shrink \( f(x) \) (reward).
- Since you're optimizing P, it's in the numerator.

---

## ✅ GRPO’s Rewarded Log-Ratio Trick

If:
- Reward \( r(x) \) is high
- \( \pi_\theta(x) \gg \pi_{\text{ref}}(x) \)

Then:
- The log-ratio is positive
- Reward is positive
- Loss = \( -r(x) \cdot \log(\pi_\theta / \pi_{\text{ref}}) \) is **negative**
- We minimize the loss → we **reinforce** this behavior.

✅ So GRPO **encourages divergence from the reference** model **only if the reward justifies it**.

🧠 This allows new behaviors to emerge without needing to balance a KL penalty manually.

---

## ⚠️ Important Notes

- We **do not** take argmax from both models — that would compare different tokens.
- We **only evaluate** the sequence generated by the current model (πθ).
- For each token \( x_t \):
  - Take \( \log \pi_\theta(x_t \mid x_{<t}) \)
  - Take \( \log \pi_{\text{ref}}(x_t \mid x_{<t}) \)
  - Compute the log-ratio.
- Both models must share the **same vocabulary index** for this to work.